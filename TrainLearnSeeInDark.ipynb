{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main File\n",
    "## This is the Main file to Train the network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import time\n",
    "import torch\n",
    "from torch import optim\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import visdom\n",
    "import rawpy\n",
    "import glob\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model - Based on U-Net Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LeakyReLU(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(LeakyReLU, self).__init__()\n",
    "\n",
    "    def forward(self, x):\n",
    "        return torch.max(x * 0.2, x)\n",
    "\n",
    "class UNetConvBlock(nn.Module):\n",
    "\n",
    "    def __init__(self, in_channel, out_channel):\n",
    "        super(UNetConvBlock, self).__init__()\n",
    "        self.UNetConvBlock = torch.nn.Sequential(\n",
    "            nn.Conv2d(in_channels=in_channel, out_channels=out_channel, kernel_size=3, padding=1),\n",
    "            LeakyReLU(),\n",
    "            nn.Conv2d(in_channels=out_channel, out_channels=out_channel, kernel_size=3, padding=1),\n",
    "            LeakyReLU()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.UNetConvBlock(x)\n",
    "\n",
    "class UNet(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(UNet, self).__init__()\n",
    "        self.conv1 = UNetConvBlock(4, 32)   #We have 4 Channel (R, G, B G)- Bayer Pattern Input\n",
    "        self.conv2 = UNetConvBlock(32, 64)\n",
    "        self.conv3 = UNetConvBlock(64, 128)\n",
    "        self.conv4 = UNetConvBlock(128, 256)\n",
    "        self.conv5 = UNetConvBlock(256, 512)\n",
    "        self.up6 = nn.ConvTranspose2d(512, 256, kernel_size=2, stride=2)\n",
    "        self.conv6 = UNetConvBlock(512, 256)\n",
    "        self.up7 = nn.ConvTranspose2d(256, 128, kernel_size=2, stride=2)\n",
    "        self.conv7 = UNetConvBlock(256, 128)\n",
    "        self.up8 = nn.ConvTranspose2d(128, 64, kernel_size=2, stride=2)\n",
    "        self.conv8 = UNetConvBlock(128, 64)\n",
    "        self.up9 = nn.ConvTranspose2d(64, 32, kernel_size=2, stride=2)\n",
    "        self.conv9 = UNetConvBlock(64, 32)\n",
    "        self.conv10 = nn.Conv2d(in_channels=32, out_channels=12, kernel_size=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        conv1 = self.conv1(x)\n",
    "        pool1 = F.max_pool2d(conv1, kernel_size=2)\n",
    "\n",
    "        conv2 = self.conv2(pool1)\n",
    "        pool2 = F.max_pool2d(conv2, kernel_size=2)\n",
    "\n",
    "        conv3 = self.conv3(pool2)\n",
    "        pool3 = F.max_pool2d(conv3, kernel_size=2)\n",
    "\n",
    "        conv4 = self.conv4(pool3)\n",
    "        pool4 = F.max_pool2d(conv4, kernel_size=2)\n",
    "\n",
    "        conv5 = self.conv5(pool4)\n",
    "\n",
    "        up6 = self.up6(conv5)\n",
    "        up6 = torch.cat([up6, conv4], 1)\n",
    "        conv6 = self.conv6(up6)\n",
    "\n",
    "        up7 = self.up7(conv6)\n",
    "        up7 = torch.cat([up7, conv3], 1)\n",
    "        conv7 = self.conv7(up7)\n",
    "        \n",
    "        up8 = self.up8(conv7)\n",
    "        up8 = torch.cat([up8, conv2], 1)\n",
    "        conv8 = self.conv8(up8)\n",
    "\n",
    "        up9 = self.up9(conv8)\n",
    "        up9 = torch.cat([up9, conv1], 1)\n",
    "        conv9 = self.conv9(up9)\n",
    "\n",
    "        conv10 = self.conv10(conv9)\n",
    "        out = F.pixel_shuffle(conv10, 2)\n",
    "\n",
    "        return out\n",
    "\n",
    "    def _initialize_weights(self):\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                m.weight.data.normal_(0.0, 0.02)\n",
    "                if m.bias is not None:\n",
    "                    m.bias.data.normal_(0.0, 0.02)\n",
    "            if isinstance(m, nn.ConvTranspose2d):\n",
    "                m.weight.data.normal_(0.0, 0.02)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Location of Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "ShortExposure = './Sony/short/'\t#Training Data\n",
    "LongExposure = './Sony/long/'\t#Referrance Data\n",
    "ResultFolder = './Results/'\t\t\t#Save result and model\n",
    "listImage = glob.glob(LongExposure + '0*.ARW')\n",
    "imageList = [int(os.path.basename(singleImage)[0:5]) for singleImage in listImage]\n",
    "\n",
    "PatchSize = 512  \t\t\t\t\t\t# 512X512 size is considered for training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Detarming Black Lebel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "512 16383\n"
     ]
    }
   ],
   "source": [
    "imgBlack = rawpy.imread('./Sony/short/00001_00_0.04s.ARW')\n",
    "BlackCh = imgBlack.black_level_per_channel[0]\n",
    "BlackMax = np.max(imgBlack.raw_image)\n",
    "print(BlackCh, BlackMax)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert Bayer pattern 4 channels R,G,B,G before passing to U-Net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rgbg(imgRaw):\n",
    "    img = imgRaw.raw_image_visible.astype(np.float32)\n",
    "    img = np.maximum(img - BlackCh, 0) / (BlackMax - BlackCh)\n",
    "    img = np.expand_dims(img, axis=2)\n",
    "    S0, S1 = img.shape[0], img.shape[1]\n",
    "\n",
    "    grbgCh = np.concatenate((img[0:S0:2, 0:S1:2, :], img[0:S0:2, 1:S1:2, :], img[1:S0:2, 1:S1:2, :], img[1:S0:2, 0:S1:2, :]), axis=2)\n",
    "    return grbgCh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def randomTrue():\n",
    "    isTrue = np.random.randint(2, size=1)[0] == 1\n",
    "    return isTrue"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Determine PSNR value from two Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def psnrValue(inp, avgOut):\n",
    "\n",
    "    totalPsnr = 0 \n",
    "    Tcnt, Ch, Hig, Wid = inp.shape\n",
    "\n",
    "    for i in range(Tcnt):\n",
    "        avgOut[i] = torch.clamp(avgOut[i], min=0.0, max=1.0)\n",
    "        mse = torch.sum((inp[i] - avgOut[i])**2)/(Ch*Hig*Wid)\n",
    "        psnr =  -10*torch.log10(mse)\n",
    "        totalPsnr += psnr\n",
    "\n",
    "    AvgPsnr = totalPsnr/Tcnt\n",
    "    return AvgPsnr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### L1 Loss function. \n",
    "Can try L2, but L2 is little bit slower and doesn't provide much improvement here.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def CalcLoss(Im1, Im2):\n",
    "    lossval = torch.mean(torch.abs(Im1 - Im2))\n",
    "    return lossval"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Allocating Spaces in Memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "LongExp = [None] * len(imageList)    # Allocating spaces for Long Explsure Images. For Training Sony Dataset, it's 161.\n",
    "ShortExp = {}\n",
    "ShortExp['300'] = [None] * len(imageList)\n",
    "ShortExp['250'] = [None] * len(imageList)\n",
    "ShortExp['100'] = [None] * len(imageList)\n",
    "\n",
    "GradientLoss = np.zeros((len(imageList), 1))\n",
    "\n",
    "allfolders = glob.glob(ResultFolder + '*0')\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "U_Net = UNet()\n",
    "U_Net.to(device)\n",
    "U_Net.train()\n",
    "\n",
    "l_rate = 1e-4\n",
    "GradientOutput = optim.Adam(U_Net.parameters(), lr=l_rate)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training The Model\n",
    "Training Learning to See in dark Require 64 GB Ram. But, This program is optimized to run in 32 GB. Pleae don't run anything else. Even in my New pc it's comsuming 31GB Ram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "Epoch_Cnt = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#################################################################################################################################################################\n",
      "Epoch = 0. \tLoss = 0.08368094265460968, \tPSNR = 20.18464469909668, \tTime = 59.651766300201416\n",
      "#################################################################################################################################################################\n",
      "Epoch = 1. \tLoss = 0.06980279833078384, \tPSNR = 21.591625213623047, \tTime = 39.538325548172\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "for epoch in range(Epoch_Cnt):\n",
    "    \n",
    "    #Calculating total loss\n",
    "    etime = time.time()\n",
    "    eloss = 0\n",
    "    epsnr = 0\n",
    "\t\n",
    "    for i in np.random.permutation(len(imageList)):\n",
    "        # get the path from image id\n",
    "        ImageId = imageList[i]\n",
    "        SEimages = glob.glob(ShortExposure + '%05d_00*.ARW' % ImageId)\n",
    "        SEpath = SEimages[np.random.random_integers(0, len(SEimages) - 1)]\n",
    "        SEname = os.path.basename(SEpath)\n",
    "\n",
    "        LEimages = glob.glob(LongExposure + '%05d_00*.ARW' % ImageId)\n",
    "        LEpath = LEimages[0]\n",
    "        LEname = os.path.basename(LEpath)\n",
    "        SEexposure = float(SEname[9:-5])\n",
    "        LEexposure = float(LEname[9:-5])\n",
    "        inratio = LEexposure / SEexposure\n",
    "        Exposure = min(inratio, 300)\n",
    "\n",
    "        if ShortExp[str(Exposure)[0:3]][i] is None:\n",
    "            imgRaw = rawpy.imread(SEpath)\n",
    "            ShortExp[str(Exposure)[0:3]][i] = np.expand_dims(rgbg(imgRaw), axis=0) * Exposure\n",
    "\n",
    "            LERaw = rawpy.imread(LEpath)\n",
    "            im = LERaw.postprocess(use_camera_wb=True, half_size=False, no_auto_bright=True, output_bps=16)\n",
    "            LongExp[i] = np.expand_dims(np.float32(im / 65535.0), axis=0)\n",
    "\n",
    "\n",
    "        Dim1, Dim2 = ShortExp[str(Exposure)[0:3]][i].shape[1], ShortExp[str(Exposure)[0:3]][i].shape[2]\n",
    "        Ax1, Ax2 = np.random.randint(0, Dim2 - PatchSize), np.random.randint(0, Dim1 - PatchSize)\n",
    "        SEpatch = ShortExp[str(Exposure)[0:3]][i][:, Ax2:Ax2 + PatchSize, Ax1:Ax1 + PatchSize, :]\n",
    "        LEpatch = LongExp[i][:, Ax2 * 2:Ax2 * 2 + PatchSize * 2, Ax1 * 2:Ax1 * 2 + PatchSize * 2, :]\n",
    "        \n",
    "        if randomTrue():  # random flip\n",
    "            SEpatch = np.flip(SEpatch, axis=1)\n",
    "            LEpatch = np.flip(LEpatch, axis=1)\n",
    "        if randomTrue():\n",
    "            SEpatch = np.flip(SEpatch, axis=2)\n",
    "            LEpatch = np.flip(LEpatch, axis=2)\n",
    "        if randomTrue():  # random transpose\n",
    "            SEpatch = np.transpose(SEpatch, (0, 2, 1, 3))\n",
    "            LEpatch = np.transpose(LEpatch, (0, 2, 1, 3))\n",
    "\n",
    "        SEpatch, LEpatch = np.minimum(SEpatch, 1.0), np.maximum(LEpatch, 0.0)\n",
    "        ImageIn = torch.from_numpy(SEpatch).permute(0,3,1,2).to(device)\n",
    "        LEimageOut = torch.from_numpy(LEpatch).permute(0,3,1,2).to(device)\n",
    "\n",
    "        GradientOutput.zero_grad()\n",
    "        ImageOut = U_Net(ImageIn)\n",
    "\n",
    "        final = ImageOut.permute(0, 2, 3, 1).cpu().data.numpy()\n",
    "        final = np.minimum(np.maximum(final,0),1)\n",
    "\n",
    "        loss = CalcLoss(ImageOut, LEimageOut)\n",
    "        eloss = eloss+loss #Total Loss\n",
    "        PSNR = psnrValue(ImageOut, LEimageOut)\n",
    "        epsnr = epsnr+PSNR #Total psnr\n",
    "        \n",
    "        loss.backward()\n",
    "        GradientOutput.step()\n",
    "        GradientLoss[i] = loss.item()\n",
    "\n",
    "        print(\"#\", end=\"\")\n",
    "    \n",
    "    \n",
    "    # Saving Snapshot of Model with different name for each 100 epoch\n",
    "    if np.mod(epoch, 100):\n",
    "        ModelName = ResultFolder + \"ModelSnapshot_\"+str(epoch)+\"_epoch.pth\"\n",
    "        torch.save(U_Net.state_dict(), ModelName)\n",
    "        \n",
    "    # Saving Snapshot of Model with different name for each 100 epoch\n",
    "    if np.mod(epoch, 5):\n",
    "        torch.save(U_Net.state_dict(), ResultFolder + 'ModelSnapshot.pth')\n",
    "    \n",
    "    #Calculate Average Loss & PSNR\n",
    "    esize = len(imageList)\n",
    "    aloss = eloss/esize\n",
    "    apsnr = epsnr/esize\n",
    "    \n",
    "    print(f\"\\nEpoch = {epoch}. \\tLoss = {aloss}, \\tPSNR = {apsnr}, \\tTime = {time.time() - etime}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
